{"container_type": "Author", "filled": ["basics", "publications", "indices", "counts"], "scholar_id": "6TuPwzMAAAAJ&hl=en\n", "source": "AUTHOR_PROFILE_PAGE", "name": "Zhenhong Zhou", "affiliation": "Beijing University of Posts and Telecommunications", "organization": 8426264526679338639, "interests": ["Large Language Model", "AI Safety", "LLM Safety"], "email_domain": "@bupt.edu.cn", "homepage": "https://ydyjya.github.io/", "citedby": 37, "publications": {"6TuPwzMAAAAJ:zYLM7Y9cAGgC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States", "pub_year": "2024"}, "filled": false, "author_pub_id": "6TuPwzMAAAAJ:zYLM7Y9cAGgC", "num_citations": 14, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1704713256080794867", "cites_id": ["1704713256080794867"]}, "6TuPwzMAAAAJ:qjMakFHDy7sC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue", "pub_year": "2024"}, "filled": false, "author_pub_id": "6TuPwzMAAAAJ:qjMakFHDy7sC", "num_citations": 11, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=246666116645463275", "cites_id": ["246666116645463275"]}, "6TuPwzMAAAAJ:2osOgNQ5qMEC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Quantifying and Analyzing Entity-Level Memorization in Large Language Models", "pub_year": "2024"}, "filled": false, "author_pub_id": "6TuPwzMAAAAJ:2osOgNQ5qMEC", "num_citations": 7, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=13708066824123900276", "cites_id": ["13708066824123900276"]}, "6TuPwzMAAAAJ:Tyk-4Ss8FVUC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Course-correction: Safety alignment using synthetic preferences", "pub_year": "2024"}, "filled": false, "author_pub_id": "6TuPwzMAAAAJ:Tyk-4Ss8FVUC", "num_citations": 3, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=15627616108519400285", "cites_id": ["15627616108519400285"]}, "6TuPwzMAAAAJ:YsMSGLbcyi4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Alignment-Enhanced Decoding: Defending Jailbreaks via Token-Level Adaptive Refining of Probability Distributions", "pub_year": "2024"}, "filled": false, "author_pub_id": "6TuPwzMAAAAJ:YsMSGLbcyi4C", "num_citations": 1, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=11641337136650603014,14683325261242386312", "cites_id": ["11641337136650603014", "14683325261242386312"]}, "6TuPwzMAAAAJ:UeHWp8X0CEIC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Enforcing group fairness in privacy-preserving Federated Learning", "pub_year": "2024"}, "filled": false, "author_pub_id": "6TuPwzMAAAAJ:UeHWp8X0CEIC", "num_citations": 1, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=11216692450238840197", "cites_id": ["11216692450238840197"]}, "6TuPwzMAAAAJ:eQOLeE2rZwMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Crabs: Consuming Resrouce via Auto-generation for LLM-DoS Attack under Black-box Settings", "pub_year": "2024"}, "filled": false, "author_pub_id": "6TuPwzMAAAAJ:eQOLeE2rZwMC", "num_citations": 0}, "6TuPwzMAAAAJ:W7OEmFMy1HYC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "On the Role of Attention Heads in Large Language Model Safety", "pub_year": "2024"}, "filled": false, "author_pub_id": "6TuPwzMAAAAJ:W7OEmFMy1HYC", "num_citations": 0}}, "citedby5y": 37, "hindex": 3, "hindex5y": 3, "i10index": 2, "i10index5y": 2, "cites_per_year": {"2024": 35, "2025": 2}, "updated": "2025-02-01 08:21:04.431631"}